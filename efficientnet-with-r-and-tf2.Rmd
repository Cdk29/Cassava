---
title: "Efficientnet-with-r-and-tf2"
output: html_document
---

# Efficientnet with R

This kernel is (for the moment) like euronews no comments, wihout any. I will add some as I correct the kernel or improve it.

Well, now there is some comments around the transfert learning part with tf hub, where I share my doubts and understanding around the code. 

```{r}
library(tidyverse)
library(tensorflow)
tf$executing_eagerly()
```

```{r}
library(keras)
library(reticulate)
```

```{r}
#tfhub::install_tfhub()
```


```{r}
#reticulate::py_install(c("Pillow"), pip = TRUE)
#reticulate::py_install(c("tensorflow_hub"), pip = TRUE)
library(tfhub)
```

```{r}
tensorflow::tf_version()
tfhub:::tfhub$version$`__version__`
```

```{r}
labels<-read_csv('cassava-leaf-disease-classification/train.csv')
head(labels)
```

```{r}
levels(as.factor(labels$label))
```

```{r}
idx0<-which(labels$label==0)
idx1<-which(labels$label==1)
idx2<-which(labels$label==2)
idx3<-which(labels$label==3)
idx4<-which(labels$label==4)
```

```{r}
labels$CBB<-0
labels$CBSD<-0
labels$CGM<-0
labels$CMD<-0
labels$Healthy<-0
```

```{r}
labels$CBB[idx0]<-1
labels$CBSD[idx1]<-1
labels$CGM[idx2]<-1
labels$CMD[idx3]<-1
labels$Healthy[idx4]<-1
```

```{r}
labels$label<-NULL
```

```{r}
head(labels)
```

```{r}
set.seed(6)

labels <- labels  %>% mutate(id = row_number())#Check IDs

train_labels <- labels  %>% sample_frac(.90)#Create test set
val_labels <- anti_join(labels, train_labels, by = 'id')
train_labels$id<-NULL
val_labels$id<-NULL

head(train_labels)
head(val_labels)
```

```{r}
summary(train_labels)
```

```{r}
summary(val_labels)
```

```{r}
image_path<-'cassava-leaf-disease-classification/train_images/'
```

```{r}
#data augmentation
datagen <- image_data_generator(
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.5,
  horizontal_flip = TRUE,
  fill_mode = "reflect"
)
```

```{r}
img_path<-"cassava-leaf-disease-classification/train_images/1000015157.jpg"

img <- image_load(img_path, target_size = c(448, 448))
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1, 448, 448, 3))
img_array<-img_array/255
# Generated that will flow augmented images
augmentation_generator <- flow_images_from_data(
  img_array, 
  generator = datagen, 
  batch_size = 1 
)
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
  batch <- generator_next(augmentation_generator)
  plot(as.raster(batch[1,,,]))
}
par(op)
```

```{r}
train_generator <- flow_images_from_dataframe(dataframe = train_labels, 
                                              directory = image_path,
                                              generator = datagen,
                                              class_mode = "other",
                                              x_col = "image_id",
                                              y_col = c("CBB","CBSD", "CGM", "CMD", "Healthy"),
                                              target_size = c(448, 448),
                                              batch_size=4)

validation_generator <- flow_images_from_dataframe(dataframe = val_labels, 
                                              directory = image_path,
                                              class_mode = "other",
                                              x_col = "image_id",
                                              y_col = c("CBB","CBSD", "CGM", "CMD", "Healthy"),
                                              target_size = c(448, 448),
                                              batch_size=4)
```

I did not understood at first why I could not add a layer_global_max_pooling_2d() on top of the effnet, unlike what it is done [in this kernel with an efficientnet3](https://www.kaggle.com/frlemarchand/efficientnet-aug-tf-keras-for-cassava-diseases) or [this kernel with a Resnet50](https://www.kaggle.com/cdk292/simple-resnet50-lr-finder-and-cyclic-lr-with-r). 

Looking at the output of summary(), my understanding is that the hub_layer is not exactly the same as the headless model from the Keras Applications. It is a ***feature extraction layer*** on top of which you add a new classification layer. Based on the output shape I think the last pooling operation (such as global_max_pooling2d) is already performed (and even maybe the batch normalization). 

The **definition of the feature vector** [here](https://www.tensorflow.org/hub/common_signatures/images#feature-vector) : *An image feature vector is a dense 1-D tensor that represents a whole image, typically for classification by the consumer model. (Unlike the intermediate activations of CNNs, it does not offer a spatial breakdown.*

The code below came from [this issue on github repo of tf hub for R](https://github.com/rstudio/tfhub/issues/20) and this [tutorial](https://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/). This tutorial make clear that the feature vector is the equivalent of the headless model. So basically, it is not a conv_base on which you add the head of the model, just a vector representation of the image made by the pretrained model, on which you add one layer (or two like here). That is why also you don't use this url : https://tfhub.dev/tensorflow/efficientnet/b0/classification/1 to create the classifier. Well, technically it will works, but the model/last layer will learn the classification of the plants based on the 1000 labels of image_net, as a geometric transformation of dog, cat and flowers and other labels of imagenet.

Based the [collection of effnet on tfhub](https://tfhub.dev/google/collections/efficientnet/1) I am switching on the B5 since I am using an image resolution close to its original training. The batch size is reduced in the train_generator() otherwise there is a memory problem when fine-tuning.

```{r}
input <- layer_input(shape = c(448, 448, 3))
hub_layer <- layer_hub(handle = "https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1", trainable=TRUE)
freeze_weights(hub_layer)

output <- input %>%
    layer_lambda(function(x) tensorflow::tf$image$resize(x, c(448L, 448L))) %>% 
    hub_layer %>%
    #layer_global_max_pooling_2d() %>% 
    layer_batch_normalization() %>% 
    layer_dropout(rate=0.5) %>%
    layer_dense(units=5, activation="softmax")
```

```{r}
summary(hub_layer)
```

```{r}
model <- keras_model(input, output)

summary(model)
```

```{r}
callback_lr_init <- function(logs){
      iter <<- 0
      lr_hist <<- c()
      iter_hist <<- c()
}
callback_lr_set <- function(batch, logs){
      iter <<- iter + 1
      LR <- l_rate[iter] # if number of iterations > l_rate values, make LR constant to last value
      if(is.na(LR)) LR <- l_rate[length(l_rate)]
      k_set_value(model$optimizer$lr, LR)
}

callback_lr <- callback_lambda(on_train_begin=callback_lr_init, on_batch_begin=callback_lr_set)
```

```{r}
####################
Cyclic_LR <- function(iteration=1:32000, base_lr=1e-5, max_lr=1e-3, step_size=2000, mode='triangular', gamma=1, scale_fn=NULL, scale_mode='cycle'){ # translated from python to R, original at: https://github.com/bckenstler/CLR/blob/master/clr_callback.py # This callback implements a cyclical learning rate policy (CLR). # The method cycles the learning rate between two boundaries with # some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186). # The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis. # This class has three built-in policies, as put forth in the paper. # - "triangular": A basic triangular cycle w/ no amplitude scaling. # - "triangular2": A basic triangular cycle that scales initial amplitude by half each cycle. # - "exp_range": A cycle that scales initial amplitude by gamma**(cycle iterations) at each cycle iteration. # - "sinus": A sinusoidal form cycle # # Example # > clr <- Cyclic_LR(base_lr=0.001, max_lr=0.006, step_size=2000, mode='triangular', num_iterations=20000) # > plot(clr, cex=0.2)
 
      # Class also supports custom scaling functions with function output max value of 1:
      # > clr_fn <- function(x) 1/x # > clr <- Cyclic_LR(base_lr=0.001, max_lr=0.006, step_size=400, # scale_fn=clr_fn, scale_mode='cycle', num_iterations=20000) # > plot(clr, cex=0.2)
 
      # # Arguments
      #   iteration:
      #       if is a number:
      #           id of the iteration where: max iteration = epochs * (samples/batch)
      #       if "iteration" is a vector i.e.: iteration=1:10000:
      #           returns the whole sequence of lr as a vector
      #   base_lr: initial learning rate which is the
      #       lower boundary in the cycle.
      #   max_lr: upper boundary in the cycle. Functionally,
      #       it defines the cycle amplitude (max_lr - base_lr).
      #       The lr at any cycle is the sum of base_lr
      #       and some scaling of the amplitude; therefore 
      #       max_lr may not actually be reached depending on
      #       scaling function.
      #   step_size: number of training iterations per
      #       half cycle. Authors suggest setting step_size
      #       2-8 x training iterations in epoch.
      #   mode: one of {triangular, triangular2, exp_range, sinus}.
      #       Default 'triangular'.
      #       Values correspond to policies detailed above.
      #       If scale_fn is not None, this argument is ignored.
      #   gamma: constant in 'exp_range' scaling function:
      #       gamma**(cycle iterations)
      #   scale_fn: Custom scaling policy defined by a single
      #       argument lambda function, where 
      #       0 <= scale_fn(x) <= 1 for all x >= 0.
      #       mode paramater is ignored 
      #   scale_mode: {'cycle', 'iterations'}.
      #       Defines whether scale_fn is evaluated on 
      #       cycle number or cycle iterations (training
      #       iterations since start of cycle). Default is 'cycle'.
 
      ########
      if(is.null(scale_fn)==TRUE){
            if(mode=='triangular'){scale_fn <- function(x) 1; scale_mode <- 'cycle';}
            if(mode=='triangular2'){scale_fn <- function(x) 1/(2^(x-1)); scale_mode <- 'cycle';}
            if(mode=='exp_range'){scale_fn <- function(x) gamma^(x); scale_mode <- 'iterations';}
            if(mode=='sinus'){scale_fn <- function(x) 0.5*(1+sin(x*pi/2)); scale_mode <- 'cycle';}
            if(mode=='halfcosine'){scale_fn <- function(x) 0.5*(1+cos(x*pi)^2); scale_mode <- 'cycle';}
      }
      lr <- list()
      if(is.vector(iteration)==TRUE){
            for(iter in iteration){
                  cycle <- floor(1 + (iter / (2*step_size)))
                  x2 <- abs(iter/step_size-2 * cycle+1)
                  if(scale_mode=='cycle') x <- cycle
                  if(scale_mode=='iterations') x <- iter
                  lr[[iter]] <- base_lr + (max_lr-base_lr) * max(0,(1-x2)) * scale_fn(x)
            }
      }
      lr <- do.call("rbind",lr)
      return(as.vector(lr))
}
```

```{r}
n=100
nb_epochs=10
#n_iter<-n*nb_epochs
```

```{r}
l_rate_cyclical <- Cyclic_LR(iteration=1:n, base_lr=1e-6, max_lr=1e-3, step_size=floor(n/2),
                        mode='triangular', gamma=1, scale_fn=NULL, scale_mode='cycle')

l_rate <- rep(l_rate_cyclical, nb_epochs)

plot(l_rate, type="b", pch=16, xlab="iteration", cex=0.2, ylab="learning rate", col="grey50")
```

```{r}
model %>% compile(
    optimizer=optimizer_rmsprop(lr=1e-5),
    loss="categorical_crossentropy",
    metrics = "categorical_accuracy"
)
```

The following code came from the tutorial of Keras "tutorial_save_and_restore".

```{r}
checkpoint_dir <- "checkpoints"
unlink(checkpoint_dir, recursive = TRUE)
dir.create(checkpoint_dir)
filepath <- file.path(checkpoint_dir, "eff_net_weights.{epoch:02d}.hdf5")
```

```{r}
check_point_callback <- callback_model_checkpoint(
  filepath = filepath,
  save_weights_only = TRUE,
  save_best_only = TRUE
)
```

```{r}
callback_list<-list(callback_lr, check_point_callback ) #callback to update lr
```

```{r}
history <- model %>% fit_generator(
    train_generator,
    steps_per_epoch=n,
    epochs = nb_epochs,
    callbacks = callback_list, #callback to update cylic lr
    validation_data = validation_generator,
    validation_step=20
)
```

```{r}
plot(history)
```
